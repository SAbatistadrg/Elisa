# button_locator.py

"""
üéØ BUTTON LOCATOR - Localizador Universal de Bot√µes/Elementos na Tela

FUNCIONALIDADE:
    Classe gen√©rica e reutiliz√°vel que localiza qualquer bot√£o ou elemento visual
    usando 3 t√©cnicas combinadas:

    1. Template Matching (OpenCV) - Compara imagem do bot√£o salva
    2. OCR (EasyOCR) - L√™ e localiza texto na tela
    3. LLM Vision (Ollama) - Valida com IA se encontrou corretamente

PAR√ÇMETROS EST√ÅTICOS (hardcoded):
    - margin = 100 (√°rea de valida√ß√£o LLM)
    - threshold = 0.7 (confian√ßa m√≠nima template matching)
    - confidence > 0.5 (confian√ßa m√≠nima OCR)
    - Idiomas OCR: ['pt', 'en']

PAR√ÇMETROS DIN√ÇMICOS (voc√™ passa):
    - llm_model: Modelo Ollama (padr√£o: "gemma3:12b")
    - button_name: Texto do bot√£o a procurar
    - use_template: Caminho da imagem template
    - validate_llm: True/False para valida√ß√£o com IA
    - threshold: Ajuste de confian√ßa (find_all_with_template)

M√âTODOS PRINCIPAIS:

    1. locate_tm(button_name, use_template=None, validate_llm=True)
       ‚îî‚îÄ Localiza bot√£o: Tenta Template ‚Üí OCR ‚Üí LLM
       ‚îî‚îÄ Retorna: True/False + coordenadas em self.last_found_coords

    2. find_all_with_template(screenshot, template_path, threshold=0.7)
       ‚îî‚îÄ Encontra TODOS os matches de um template (ex: m√∫ltiplos sliders)
       ‚îî‚îÄ Retorna: {'found': bool, 'matches': [{'x', 'y', 'confidence'}]}

    3. list_items_below(parent_name)
       ‚îî‚îÄ Lista itens abaixo de um elemento pai (ex: clusters abaixo de "Scans")
       ‚îî‚îÄ Retorna: dict com {nome_item: {'x': int, 'y': int}}

    4. check_text_on_screen(text)
       ‚îî‚îÄ Verifica se um texto est√° na tela usando OCR
       ‚îî‚îÄ Retorna: True se confian√ßa > 0.5, False caso contr√°rio

EXEMPLOS DE USO:

    # Inicializar
    locator = ButtonLocator(llm_model="bahtiyorovnozim/qwen3-vl-1-4b")

    # Localizar bot√£o com template + valida√ß√£o LLM
    success = locator.locate_tm(
        button_name="Registro Autom√°tico",
        use_template="./buttons/registro_automatico.png",
        validate_llm=True
    )
    if success:
        coords = locator.last_found_coords
        print(f"Bot√£o em: ({coords['x']}, {coords['y']})")

    # Encontrar todos os sliders na tela
    screenshot = locator.capture_screen()
    result = locator.find_all_with_template(
        screenshot, 
        "./buttons/slider.png",
        threshold=0.8
    )
    for match in result['matches']:
        print(f"Slider em: ({match['x']}, {match['y']})")

    # Listar clusters abaixo de "Scans"
    clusters = locator.list_items_below("Scans")
    for nome, coords in clusters.items():
        print(f"{nome}: ({coords['x']}, {coords['y']})")

    # Verificar se texto est√° na tela
    if locator.check_text_on_screen("Processando"):
        print("Texto encontrado!")
    else:
        print("Texto n√£o encontrado")

LIMITA√á√ïES:
    ‚úÖ Funciona: Bot√µes com texto, templates de imagem, elementos repetidos
    ‚ö†Ô∏è Precisa: Ter texto OU template do elemento
    ‚ö†Ô∏è OCR pode falhar: Fontes muito estilizadas ou pequenas
    ‚ö†Ô∏è LLM √© lento: Valida√ß√£o adiciona ~2-5s por bot√£o

DEPEND√äNCIAS:
    - pyautogui (captura tela)
    - easyocr (OCR)
    - ollama (LLM Vision)
    - PIL (manipula√ß√£o imagem)
    - cv2 (template matching)
    - numpy (processamento)
"""




import pyautogui
import easyocr
import ollama
from PIL import ImageGrab
import cv2
import numpy as np

class ButtonLocator:
    def __init__(self, llm_model="gemma3:12b"):
        self.llm_model = llm_model
        self.ocr_reader = easyocr.Reader(['pt', 'en'])
        self.last_found_coords = None
        self.last_ocr_coords = None

    def capture_screen(self):
        """Captura screenshot da tela"""
        screenshot = ImageGrab.grab()
        return screenshot
    

    def find_text_with_ocr(self, screenshot, target_text):
        """Encontra texto na tela e retorna coordenadas aproximadas"""
        img_array = np.array(screenshot)
        results = self.ocr_reader.readtext(img_array)
        for (bbox, text, confidence) in results:
            if target_text.lower() in text.lower():
                x_coords = [point[0] for point in bbox]
                y_coords = [point[1] for point in bbox]
                center_x = sum(x_coords) / 4
                center_y = sum(y_coords) / 4
                return {
                    'found': True,
                    'x': int(center_x),
                    'y': int(center_y),
                    'confidence': confidence,
                    'bbox': bbox
                }
        return {'found': False}

    def validate_with_llm(self, screenshot, region, button_name):
        """Valida se a regi√£o cont√©m o bot√£o usando Vision LLM"""
        x, y = region['x'], region['y']
        margin = 100
        cropped = screenshot.crop((
            max(0, x - margin),
            max(0, y - margin),
            x + margin,
            y + margin
        ))
        cropped.save('temp_region.png')
        prompt = f"Nesta imagem, existe um bot√£o com o texto '{button_name}'? Responda apenas 'sim' ou 'n√£o'."
        response = ollama.chat(
            model=self.llm_model,
            messages=[{
                'role': 'user',
                'content': prompt,
                'images': ['temp_region.png']
            }]
        )
        answer = response['message']['content'].lower()
        return 'sim' in answer

    def find_with_template(self, screenshot, template_path):
        """Tenta localizar usando template matching"""
        screen_cv = cv2.cvtColor(np.array(screenshot), cv2.COLOR_RGB2BGR)
        template = cv2.imread(template_path)

        if template is None:
            return {'found': False}

        result = cv2.matchTemplate(screen_cv, template, cv2.TM_CCOEFF_NORMED)
        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)

        if max_val > 0.7:
            template_h, template_w = template.shape[:2]
            center_x = max_loc[0] + template_w // 2
            center_y = max_loc[1] + template_h // 2
            return {
                'found': True,
                'x': center_x,
                'y': center_y,
                'confidence': max_val
            }
        return {'found': False}

    def locate_tm(self, button_name, use_template=None, validate_llm=True):
        """Localiza o bot√£o sem clicar: Template primeiro, depois OCR+LLM"""
        print(f"üîç Procurando bot√£o: {button_name}")

        screenshot = self.capture_screen()

        if use_template:
            print("üîç Tentando Template Matching...")
            template_result = self.find_with_template(screenshot, use_template)
            if template_result['found']:
                print(f"‚úì Template encontrado em: ({template_result['x']}, {template_result['y']})")
                self.last_found_coords = template_result
                return template_result
            else:
                print("‚ùå Template n√£o encontrou")

        print("üîç Tentando OCR + LLM...")
        ocr_result = self.find_text_with_ocr(screenshot, button_name)

        if not ocr_result['found']:
            print("‚ùå Texto n√£o encontrado via OCR")
            return False

        print(f"‚úì OCR encontrou em: ({ocr_result['x']}, {ocr_result['y']})")
        self.last_ocr_coords = ocr_result

        if validate_llm:
            is_valid = self.validate_with_llm(screenshot, ocr_result, button_name)
            if not is_valid:
                print("‚ùå LLM n√£o validou a regi√£o")
                return False
            print("‚úì LLM confirmou o bot√£o")

        self.last_found_coords = ocr_result
        return True
    
    def list_items_below(self, parent_name):
        """Lista todos os itens abaixo de um item pai usando LLM + OCR"""
        print(f"üîç Listando itens abaixo de '{parent_name}'...")

        screenshot = self.capture_screen()
        screenshot.save('temp_list.png')

        # LLM identifica quais itens est√£o abaixo
        prompt = f"""Nesta imagem, quais itens est√£o abaixo de '{parent_name}'? 
        Liste apenas os itens que contem "Cluster", um por linha, sem numera√ß√£o ou s√≠mbolos."""

        response = ollama.chat(
            model=self.llm_model,
            messages=[{
                'role': 'user',
                'content': prompt,
                'images': ['temp_list.png']
            }]
        )

        items_text = response['message']['content'].strip()
        items_list = [item.strip() for item in items_text.split('\n') if item.strip()]

        print(f"‚úì LLM encontrou {len(items_list)} itens: {items_list}")

        # OCR extrai coordenadas de cada item
        img_array = np.array(screenshot)
        ocr_results = self.ocr_reader.readtext(img_array)

        items_dict = {}
        for item_name in items_list:
            for (bbox, text, confidence) in ocr_results:
                if item_name.lower() in text.lower() and confidence > 0.5:
                    x_coords = [point[0] for point in bbox]
                    y_coords = [point[1] for point in bbox]
                    center_x = int(sum(x_coords) / 4)
                    center_y = int(sum(y_coords) / 4)

                    items_dict[item_name] = {
                        'x': center_x,
                        'y': center_y
                    }
        print(f"‚úì Coordenadas extra√≠das: {items_dict}")
        return items_dict
    


    def list_clusters(self):
        print(f"üîç Listando clusters da pagina inicial...")

        screenshot = self.capture_screen()
        screenshot.save('temp_list.png')

        # LLM identifica quais itens est√£o abaixo
        prompt = f"""Nesta imagem, quantos Clusters temos? Eles podem ser identificados com a seguinte 
        nomenclatura: "Cluster_2", "Cluster_34", Cluster_133". Retorne apenas os nomes de cada Cluster e nada mais."""

        response = ollama.chat(
            model=self.llm_model,
            messages=[{
                'role': 'user',
                'content': prompt,
                'images': ['temp_list.png']
            }]
        )

        items_text = response['message']['content'].strip()
        items_list = [item.strip() for item in items_text.split('\n') if item.strip()]

        print(f"‚úì LLM encontrou {len(items_list)} itens: {items_list}")

        # OCR extrai coordenadas de cada item
        img_array = np.array(screenshot)
        ocr_results = self.ocr_reader.readtext(img_array)

        items_dict = {}
        
        for item_name in items_list:
            for (bbox, text, confidence) in ocr_results:
                # Normaliza o texto do OCR corrigindo erros comuns
                if item_name.lower() in text.lower() and confidence > 0.5:
                    x_coords = [point[0] for point in bbox]
                    y_coords = [point[1] for point in bbox]
                    center_x = int(sum(x_coords) / 4)
                    center_y = int(sum(y_coords) / 4)

                    items_dict[item_name] = {
                        'x': center_x,
                        'y': center_y
                    }
        print(f"‚úì Coordenadas extra√≠das: {items_dict}")
        print(f"üìù Tudo que o OCR detectou:")
        for (bbox, text, confidence) in ocr_results:
            print(f"  - '{text}' (confian√ßa: {confidence:.2f})")
        return items_dict




    def find_all_with_template(self, screenshot, template_path, threshold=0.7):
        """
        Encontra TODOS os matches de um template na screenshot.
        Retorna uma lista com as coordenadas de todos os matches encontrados.
        """
        try:
            # Carrega a imagem e o template
            if isinstance(screenshot, str):
                img = cv2.imread(screenshot)
            else:
                img = np.array(screenshot)

            template = cv2.imread(template_path)

            if img is None or template is None:
                print(f"‚ùå Erro ao carregar imagem ou template")
                return {'found': False, 'matches': []}

            # Converte pra escala de cinza
            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            template_gray = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)

            # Template matching
            result = cv2.matchTemplate(img_gray, template_gray, cv2.TM_CCOEFF_NORMED)

            # Encontra todos os matches acima do threshold
            locations = np.where(result >= threshold)

            if len(locations[0]) == 0:
                print(f"‚ùå Nenhum match encontrado")
                return {'found': False, 'matches': []}

            # Agrupa matches pr√≥ximos (evita duplicatas)
            h, w = template_gray.shape
            matches = []

            for pt in zip(*locations[::-1]):
                x, y = pt
                confidence = result[y, x]

                # Verifica se j√° existe um match muito pr√≥ximo
                is_duplicate = False
                for existing_match in matches:
                    distance = np.sqrt((x - existing_match['x'])**2 + (y - existing_match['y'])**2)
                    if distance < w / 2:  # Se est√° muito perto, √© duplicata
                        is_duplicate = True
                        break

                if not is_duplicate:
                    matches.append({
                        'x': x + w // 2,  # Centro da bolinha
                        'y': y + h // 2,
                        'confidence': float(confidence)
                    })

            # Ordena por confian√ßa (maior primeiro)
            matches.sort(key=lambda m: m['confidence'], reverse=True)

            print(f"‚úì Encontrados {len(matches)} matches")
            for i, match in enumerate(matches):
                print(f"  Match {i+1}: ({match['x']}, {match['y']}) - Confian√ßa: {match['confidence']:.2%}")

            return {'found': True, 'matches': matches}

        except Exception as e:
            print(f"‚ùå Erro em find_all_with_template: {e}")
            return {'found': False, 'matches': []}


    def read_report(self):
        screenshot = self.capture_screen()
        screenshot.save('temp_list.png')

        # LLM identifica quais itens est√£o abaixo
        #prompt = f"""Nesta imagem, procure os valores descrito nos outputs: 'Erro de ponto m√°ximo' em mm, 'Media de erro de ponto' em mm e 
        #'Sobreposi√ß√£o m√≠nima' em %. Retorne apenas os n√∫mero no formato float. Exemplo: 1.7, 3.5, 25.0. Caso
        #n√£o tenha certeza de algum valor, retorne -1.0. Retorne apenas os n√∫meros e nada mais"""

        prompt = """Me diga o valor m√°ximo, M√©dio e percentual que voc√™ v√™ na tela. Retorne apenas os numeros e nada mais."""
        response = ollama.chat(
            model=self.llm_model,
            messages=[{
                'role': 'user',
                'content': prompt,
                'images': ['temp_list.png']
            }]
        )

        items_text = response['message']['content'].strip()
        items_list = [item.strip() for item in items_text.split('\n') if item.strip()]

        print(f"‚úì LLM encontrou {len(items_list)} itens: {items_list}")
        return items_list